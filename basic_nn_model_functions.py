# -*- coding: utf-8 -*-
"""basic_nn_model_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dOsnsXv-AFvlHRpL228d5iB6Skv9dbjB
"""

# Module imports
import numpy as np
import copy

"""Functions"""

def neural_network_size(num_layers, num_nodes):
    """
    Arguments:
    num_layers -- number of desired hidden layers in neural network
    num_nodes -- a numpy array to indicate the number of nodes in the layers (shape: (1, num_layers))
    """
    # Set desired number of hidden layers:
    n_h = num_layers

    # Set a vector containing the desired number of nodes in each hidden layer:
    n_n = num_nodes  # Assuming num_nodes is already a numpy array of shape (1, num_layers)

    return n_h, n_n

def layer_sizes(X, Y):
    """
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)

    Returns:
    n_x -- the size of the input layer
    n_y -- the size of the output layer
    """
    # Determine size of input and output layers
    n_x = X.shape[0]
    n_y = Y.shape[0]

    return (n_x, n_y)

def initialize_parameters(n_x, n_h, n_n, n_y):
    """
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_n -- numpy array containing the number of nodes in each hidden layer (shape: (1, num_layers))
    n_y -- size of the output layer

    Returns:
    params -- python dictionary containing your parameters
    """
    # Check that n_h is equal to the number of columns in n_n
    assert n_h == n_n.shape[1], "n_h must equal the number of columns in n_n"

    parameters = {}

    # Initialize parameters for the hidden layers
    for i in range(1, n_h + 2):
        if i == 1:
            # First hidden layer (connected to the input layer)
            parameters[f"W{i}"] = np.random.randn(n_n[0][0], n_x) * 0.01
            parameters[f"b{i}"] = np.zeros((n_n[0][0], 1))
        elif i == n_h + 1:
            # Output layer (connected to the last hidden layer)
            parameters[f"W{i}"] = np.random.randn(n_y, n_n[0][-1]) * 0.01
            parameters[f"b{i}"] = np.zeros((n_y, 1))
        else:
            # Intermediate hidden layers
            parameters[f"W{i}"] = np.random.randn(n_n[0][i-1], n_n[0][i-2]) * 0.01
            parameters[f"b{i}"] = np.zeros((n_n[0][i-1], 1))

    return parameters

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def forward_propagation(X, parameters):
    """
    Argument:
    X -- input data of size (n_x, m) where m is the number of training examples
    parameters -- python dictionary containing your parameters (output of initialization function)

    Returns:
    A_last -- The sigmoid output of the last activation
    cache -- a dictionary containing "Z" and "A" for each layer
    """
    cache = {}
    A_prev = X  # Initialize A_prev as the input data

    # Number of layers (excluding input layer)
    num_layers = len(parameters) // 2

    # Iterate through the hidden layers
    for i in range(1, num_layers):
        W = parameters[f"W{i}"]
        b = parameters[f"b{i}"]
        Z = np.matmul(W, A_prev) + b
        A = np.tanh(Z)
        cache[f"Z{i}"] = Z
        cache[f"A{i}"] = A
        A_prev = A

    # Final layer (output layer) with sigmoid activation
    W_last = parameters[f"W{num_layers}"]
    b_last = parameters[f"b{num_layers}"]
    Z_last = np.matmul(W_last, A_prev) + b_last
    A_last = sigmoid(Z_last)

    cache[f"Z{num_layers}"] = Z_last
    cache[f"A{num_layers}"] = A_last

    assert(A_last.shape == (1, X.shape[1]))

    return A_last, cache

def compute_cost(A_last, Y):
    """
    Computes the cross-entropy cost given in equation (13)

    Arguments:
    A_last -- The sigmoid output of the last activation, of shape (1, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)

    Returns:
    cost -- cross-entropy cost given equation
    """
    # obtain number of examples
    m = Y.shape[1]

    # Compute the cross-entropy cost
    logprobs = np.multiply(Y, np.log(A_last)) + np.multiply((1 - Y), np.log(1 - A_last))
    cost = - np.sum(logprobs) / m

    # Make sure cost is the dimension we expect.
    cost = float(np.squeeze(cost)) # e.g., turns [[17]] into 17

    return cost

def backward_propagation(parameters, cache, X, Y):
    """
    Arguments:
    parameters -- python dictionary containing our parameters
    cache -- a dictionary containing "Z" and "A" for each layer
    X -- input data of shape (n_x, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)

    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    """
    m = X.shape[1]
    num_layers = len(parameters) // 2
    grads = {}

    # Retrieve the last activation from the cache
    A_last = cache[f"A{num_layers}"]

    # Compute the gradient of the loss with respect to the last activation
    dZ_last = A_last - Y
    dW_last = (1/m) * np.matmul(dZ_last, cache[f"A{num_layers - 1}"].T)
    db_last = (1/m) * np.sum(dZ_last, axis=1, keepdims=True)

    # Store the gradients for the last layer
    grads[f"dW{num_layers}"] = dW_last
    grads[f"db{num_layers}"] = db_last

    # Iterate through the hidden layers in reverse order
    dZ_next = dZ_last
    for i in range(num_layers - 1, 0, -1):
        dZ = np.matmul(parameters[f"W{i + 1}"].T, dZ_next) * (1 - np.power(cache[f"A{i}"], 2))
        dW = (1/m) * np.matmul(dZ, cache[f"A{i - 1}"].T) if i > 1 else (1/m) * np.matmul(dZ, X.T)
        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)

        # Store the gradients for the current layer
        grads[f"dW{i}"] = dW
        grads[f"db{i}"] = db

        dZ_next = dZ

    return grads

def update_parameters(parameters, grads, learning_rate=1.2):
    """
    Updates parameters using the gradient descent update rule

    Arguments:
    parameters -- python dictionary containing your parameters
    grads -- python dictionary containing your gradients

    Returns:
    parameters -- python dictionary containing your updated parameters
    """
    # Determine the number of layers (excluding input layer)
    num_layers = len(parameters) // 2

    # Update rule for each parameter
    for i in range(1, num_layers + 1):
        W = copy.deepcopy(parameters[f"W{i}"])
        b = copy.deepcopy(parameters[f"b{i}"])
        dW = grads[f"dW{i}"]
        db = grads[f"db{i}"]

        # Update weights and biases
        parameters[f"W{i}"] = W - learning_rate * dW
        parameters[f"b{i}"] = b - learning_rate * db

    return parameters

def nn_model(X, Y, n_h, n_n, alpha, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- Number of hidden layers
    n_n -- A numpy array of shape (1, n_h) containing the number of nodes in each hidden layer
    alpha -- the learning rate
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations

    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """

    np.random.seed(3)
    n_x, n_y = layer_sizes(X, Y)

    # Initialize parameters
    parameters = initialize_parameters(n_x, n_h, n_n, n_y)

    # Loop (gradient descent)
    for i in range(0, num_iterations):

        # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".
        # A_last, cache = ...

        # Cost function. Inputs: "A_last, Y". Outputs: "cost".
        # cost = ...

        # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".
        # grads = ...

        # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".
        # parameters = ...

        A_last, cache = forward_propagation(X, parameters)
        cost = compute_cost(A_last, Y)
        grads = backward_propagation(parameters, cache, X, Y)
        parameters = update_parameters(parameters, grads, learning_rate = alpha)

        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    return parameters

def predict(nn_parameters, X, decis_thresh):
    """
    Using the learned parameters, predicts a class for each example in X

    Arguments:
    parameters -- python dictionary containing your parameters
    X -- input data of size (n_x, m)

    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    """
    # Computes probabilities using forward propagation, and classifies to 0/1 using decis_thresh as the threshold.
    A_last, cache = forward_propagation(X, nn_parameters)
    predictions = (A_last > decis_thresh)

    return predictions

def check_data_shapes(X, Y):
    X_m = X.shape[1]
    Y_m = Y.shape[1]
    assert X_m == Y_m, "X_m and Y_m must be the same size"

def check_neural_network_size(num_layers, num_nodes):
    n_h, n_n = neural_network_size(num_layers, num_nodes)
    assert n_h == n_n.shape[1], "n_h must equal the number of columns in n_n"

def check_last_activation(A_last, cache, num_layers):
    # Construct the key for the last activation
    last_activation_key = f"A{num_layers}"

    # Retrieve the last activation from the cache
    A_last_from_cache = cache[last_activation_key]

    # Check if A_last is equal to the last activation from the cache
    is_equal = np.allclose(A_last, A_last_from_cache)
    assert is_equal, "A_last must be equal to the last activation from the cache"

def compute_accuracy(nn_parameters, X, Y, decis_thresh):
    """
    Computes and prints the accuracy of the model.

    Arguments:
    nn_parameters -- python dictionary containing the learned parameters
    X -- input data of size (n_x, m)
    Y -- "true" labels vector of shape (1, number of examples)
    decis_thresh -- decision threshold for classifying predictions

    Returns:
    accuracy -- the accuracy of the model as a percentage
    """
    predictions = predict(nn_parameters, X, decis_thresh)
    accuracy = float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100)
    print('Accuracy:', accuracy, '%')
    return accuracy